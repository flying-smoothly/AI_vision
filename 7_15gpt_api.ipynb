{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "18fIWo7OxDM0i8mgnAF0spNaQgKgJk_NM",
      "authorship_tag": "ABX9TyOFS51GDBn5NSBXXh+lYdv0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/flying-smoothly/AI_vision/blob/master/7_15gpt_api.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip uninstall openai"
      ],
      "metadata": {
        "id": "w9fBK4tfonLS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9711587-4509-4056-e6d9-fb77cef24851"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping openai as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai"
      ],
      "metadata": {
        "id": "qJfvECCpoqGt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0a056a9-eaa4-4b7d-8329-33c44b038df6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.37.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Downloading openai-1.37.2-py3-none-any.whl (337 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.1/337.1 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.37.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "id": "A7lBd9ALos-2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01b13e9a-33fe-4653-a7c5-587c6f4bed29"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install opencv-python\n",
        "pip install opencv-python-headless"
      ],
      "metadata": {
        "id": "CH_HmZqMiq8q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "27edb417-53e5-4481-eb80-8f606057b281"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-4-75d6990a88f1>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-75d6990a88f1>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pip install opencv-python\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "import requests\n",
        "from PIL import Image, ImageDraw\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "from io import BytesIO\n",
        "import base64\n",
        "from dotenv import load_dotenv, find_dotenv"
      ],
      "metadata": {
        "id": "9zASAvFjovPS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCclAMKheboT",
        "outputId": "32370a07-b6c9-46c6-9be2-3af802193793"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "load_dotenv('/content/drive/MyDrive/key.env')\n",
        "\n",
        "openai.api_key = os.getenv('OPENAI_API_KEY') #os 모듈을 사용하여 환경 변수에서 API 키를 불러옵니다.\n",
        "\n",
        "if openai.api_key:\n",
        "    print(\"API 키가 성공적으로 설정되었습니다.\")\n",
        "else:\n",
        "    print(\"API 키를 설정하는 데 실패했습니다.\")\n",
        "\n",
        "client=OpenAI()"
      ],
      "metadata": {
        "id": "LQaUJaj9oxbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38029506-8a79-4dd1-9c7b-71ac4c545626"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API 키가 성공적으로 설정되었습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9DTJBmN_ol4m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6ba44e3-43e8-4a64-d9ea-de114501a56f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'chatcmpl-9rg4DJYk3y8VkiHCbTqgslZ9jXqJQ', 'object': 'chat.completion', 'created': 1722580117, 'model': 'gpt-4o-2024-05-13', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '제공된 이미지는 깊이 카메라를 사용하여 촬영된 것으로 보이며, 색상은 심도 정보를 나타내는 데 사용됩니다. 일반적으로 이러한 이미지는 색상 맵을 사용하여 걸린 근사 거리를 나타냅니다. 이미지의 색상은 다음과 같은 일반적인 깊이 정보 매핑을 따를 수 있습니다:\\n- 빨간색: 멀리 있는 물체\\n- 파란색: 가까이 있는 물체\\n\\n이미지에서 스프레이의 위치를 명확히 알 수 없지만, 전체적으로 이미지의 중앙 부분이 녹색과 노란색으로 보이며 이는 중간 정도의 거리를 나타낼 수 있습니다. 반면, 하단의 파란색 부분은 카메라에 매우 가까운 거리를, 상단의 빨간색 부분은 매우 먼 거리를 나타냅니다.\\n\\n따라서, 스프레이가 무엇인지, 어떤 색으로 나타나는지 확실하지 않기 때문에 구체적인 거리를 알 수 없습니다. 이미지에서 중앙 영역의 물체가 일반적으로 중간 거리임을 감안할 때, 대략적인 거리를 추정할 수 있습니다.\\n\\n정확한 거리를 측정하려면 심도 카메라의 보정 데이터와 매핑 정보가 필요합니다. 이는 심도 이미지 내 픽셀의 색상 값과 실제 거리를 매핑한 데이터입니다. 만약 해당 정보를 알고 있다면'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 343, 'completion_tokens': 300, 'total_tokens': 643}, 'system_fingerprint': 'fp_bc2a86f5f5'}\n"
          ]
        }
      ],
      "source": [
        "# Function to encode the image\n",
        "def encode_image(image_path):\n",
        "  with open(image_path, \"rb\") as image_file:\n",
        "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "# Path to your image\n",
        "image_path = \"/content/drive/MyDrive/depth.bag/depth_Depth_31135.png\"\n",
        "\n",
        "# Getting the base64 string\n",
        "base64_image = encode_image(image_path)\n",
        "\n",
        "headers = {\n",
        "  \"Content-Type\": \"application/json\",\n",
        "  \"Authorization\": f\"Bearer {openai.api_key}\"\n",
        "}\n",
        "\n",
        "payload = {\n",
        "  \"model\": \"gpt-4o\",\n",
        "  \"messages\": [\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": [\n",
        "        {\n",
        "          \"type\": \"text\",\n",
        "          \"text\": \"png 파일에 depth camera로 찍은 이미지의 깊이 정보와 RGB 정보가 저장되어 있어. 이미지는 스프레이를 찍은 거고 나는 스프레이의 깊이 정보를 알고 싶어. 스프레이가 카메라로부터 얼마나 떨어져 있었는지 png 파일을 읽어서 말해줄 수 있어? 얼마나 떨어져 있는지만 말해주면 돼.\"\n",
        "        },\n",
        "        {\n",
        "          \"type\": \"image_url\",\n",
        "          \"image_url\": {\n",
        "            \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
        "          }\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  ],\n",
        "  \"max_tokens\": 300\n",
        "}\n",
        "\n",
        "response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
        "\n",
        "print(response.json())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################################\n",
        "##               Read bag from file                ##\n",
        "#####################################################\n",
        "\n",
        "\n",
        "# First import library\n",
        "import pyrealsense2 as rs\n",
        "# Import Numpy for easy array manipulation\n",
        "import numpy as np\n",
        "# Import OpenCV for easy image rendering\n",
        "import cv2\n",
        "# Import argparse for command-line options\n",
        "import argparse\n",
        "# Import os.path for file path manipulation\n",
        "import os.path\n",
        "\n",
        "# Create object for parsing command-line options\n",
        "parser = argparse.ArgumentParser(description=\"Read recorded bag file and display depth stream in jet colormap.\\\n",
        "                                Remember to change the stream fps and format to match the recorded.\")\n",
        "# Add argument which takes path to a bag file as an input\n",
        "parser.add_argument(\"-i\", \"--input\", type=str, help=\"Path to the bag file\")\n",
        "# Parse the command line arguments to an object\n",
        "args = parser.parse_args()\n",
        "# Safety if no parameter have been given\n",
        "if not args.input:\n",
        "    print(\"No input paramater have been given.\")\n",
        "    print(\"For help type --help\")\n",
        "    exit()\n",
        "# Check if the given file have bag extension\n",
        "if os.path.splitext(args.input)[1] != \".bag\":\n",
        "    print(\"The given file is not of correct file format.\")\n",
        "    print(\"Only .bag files are accepted\")\n",
        "    exit()\n",
        "try:\n",
        "    # Create pipeline\n",
        "    pipeline = rs.pipeline()\n",
        "\n",
        "    # Create a config object\n",
        "    config = rs.config()\n",
        "\n",
        "    # Tell config that we will use a recorded device from file to be used by the pipeline through playback.\n",
        "    rs.config.enable_device_from_file(config, args.input)\n",
        "\n",
        "    # Configure the pipeline to stream the depth stream\n",
        "    # Change this parameters according to the recorded bag file resolution\n",
        "    config.enable_stream(rs.stream.depth, rs.format.z16, 30)\n",
        "\n",
        "    # Start streaming from file\n",
        "    pipeline.start(config)\n",
        "\n",
        "    # Create opencv window to render image in\n",
        "    cv2.namedWindow(\"Depth Stream\", cv2.WINDOW_AUTOSIZE)\n",
        "\n",
        "    # Create colorizer object\n",
        "    colorizer = rs.colorizer()\n",
        "\n",
        "    # Streaming loop\n",
        "    while True:\n",
        "        # Get frameset of depth\n",
        "        frames = pipeline.wait_for_frames()\n",
        "\n",
        "        # Get depth frame\n",
        "        depth_frame = frames.get_depth_frame()\n",
        "\n",
        "        # Colorize depth frame to jet colormap\n",
        "        depth_color_frame = colorizer.colorize(depth_frame)\n",
        "\n",
        "        # Convert depth_frame to numpy array to render image in opencv\n",
        "        depth_color_image = np.asanyarray(depth_color_frame.get_data())\n",
        "\n",
        "        # Render image in opencv window\n",
        "        cv2.imshow(\"Depth Stream\", depth_color_image)\n",
        "        key = cv2.waitKey(1)\n",
        "        # if pressed escape exit program\n",
        "        if key == 27:\n",
        "            cv2.destroyAllWindows()\n",
        "            break"
      ],
      "metadata": {
        "id": "naqQoGPBqJ8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# 이미지 읽기\n",
        "image_path = '/content/drive/MyDrive/LLM/cat6.jpeg'\n",
        "image = cv2.imread(image_path)\n",
        "\n",
        "# 이미지가 제대로 로드되었는지 확인\n",
        "if image is None:\n",
        "    print(\"Error: Unable to load image. Please check the file path.\")\n",
        "else:\n",
        "    # 예시로 설정한 컵과 손의 바운딩 박스 좌표 (직접 정의 필요)\n",
        "    cat_box = (96, 90, 247, 426)  # 고양이 바운딩 박스 좌표\n",
        "    #hand_box = (830, 700, 960, 820)  # 손 바운딩 박스 좌표\n",
        "\n",
        "    # 바운딩 박스 그리기\n",
        "    cv2.rectangle(image, (cat_box[0], cat_box[1]), (cat_box[2], cat_box[3]), (0, 255, 0), 2)\n",
        "    #cv2.rectangle(image, (hand_box[0], hand_box[1]), (hand_box[2], hand_box[3]), (255, 0, 0), 2)\n",
        "\n",
        "    # 결과 이미지 보여주기\n",
        "    cv2_imshow(image)"
      ],
      "metadata": {
        "id": "BecNJwcxe3CR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지를 읽어옵니다\n",
        "image_path = '/content/drive/MyDrive/LLM/cat1.jpg'\n",
        "image = cv2.imread(image_path)\n",
        "\n",
        "# 이미지가 제대로 로드되었는지 확인합니다\n",
        "if image is None:\n",
        "    raise ValueError(f\"이미지를 로드할 수 없습니다: {image_path}\")\n",
        "\n",
        "# 이미지를 그레이스케일로 변환합니다\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# 노이즈 제거를 위해 Gaussian 블러링을 적용합니다\n",
        "blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "\n",
        "# 이진화하여 컵을 추출합니다\n",
        "_, thresh = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "\n",
        "# 모폴로지 연산을 사용하여 객체를 확장합니다\n",
        "kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))\n",
        "morphed = cv2.dilate(thresh, kernel, iterations=2)\n",
        "\n",
        "# 외곽선을 찾습니다\n",
        "contours, _ = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "# 각 외곽선을 감싸는 직사각형을 계산하고 이미지에 그립니다\n",
        "for i, contour in enumerate(contours):\n",
        "    x, y, w, h = cv2.boundingRect(contour)\n",
        "    # 필터링 조건: 특정 크기 이상의 컨투어만 처리합니다\n",
        "    if w > 50 and h > 50:\n",
        "        # 바운딩 박스를 약간 확장합니다\n",
        "        padding = 10\n",
        "        x, y, w, h = x - padding, y - padding, w + 2 * padding, h + 2 * padding\n",
        "\n",
        "        # 정규화된 좌표 출력\n",
        "        print(f\"Cup {i+1}:\")\n",
        "        print(f\"Top-left: ({x}, {y})\")\n",
        "        print(f\"Top-right: ({x+w}, {y})\")\n",
        "        print(f\"Bottom-left: ({x}, {y + h})\")\n",
        "        print(f\"Bottom-right: ({x + w}, {y + h})\")\n",
        "        print()\n",
        "\n",
        "        # 원본 이미지에 사각형 그리기\n",
        "        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "\n",
        "# 이미지를 출력합니다\n",
        "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "plt.axis('off')  # 축을 끕니다\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-DkeXNNTi3Tk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}